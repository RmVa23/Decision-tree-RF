{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import *\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.metrics import specificity_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r\"C:\\Users\\rmace\\Repo. Github\\Decision-tree-RF\\data\\clean-data (1).csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(\"Outcome\", axis = 1)\n",
    "y = data[\"Outcome\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBClassifier(n_estimators = 300, learning_rate = 0.005, random_state = 42)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar el primer modelo en entrenamiento y prueba\n",
    "train_pred = model.predict(X_train)\n",
    "test_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(y_train, y_test, y_pred_train, y_pred_test):\n",
    "    # Calcular métricas para el conjunto de entrenamiento\n",
    "    train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "    train_f1 = f1_score(y_train, y_pred_train)\n",
    "    train_auc = roc_auc_score(y_train, y_pred_train)\n",
    "    train_precision = precision_score(y_train, y_pred_train)\n",
    "    train_recall = recall_score(y_train, y_pred_train)\n",
    "    train_specificity = specificity_score(y_train, y_pred_train)\n",
    "\n",
    "    # Calcular métricas para el conjunto de prueba\n",
    "    test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "    test_f1 = f1_score(y_test, y_pred_test)\n",
    "    test_auc = roc_auc_score(y_test, y_pred_test)\n",
    "    test_precision = precision_score(y_test, y_pred_test)\n",
    "    test_recall = recall_score(y_test, y_pred_test)\n",
    "    test_specificity = specificity_score(y_test, y_pred_test)\n",
    "\n",
    "    # Calcular la diferencia entre métricas de entrenamiento y prueba\n",
    "    diff_accuracy = train_accuracy - test_accuracy\n",
    "    diff_f1 = train_f1 - test_f1\n",
    "    diff_auc = train_auc - test_auc\n",
    "    diff_precision = train_precision - test_precision\n",
    "    diff_recall = train_recall - test_recall\n",
    "    diff_specificity = train_specificity - test_specificity\n",
    "\n",
    "    # Crear un DataFrame con los resultados\n",
    "    metrics_df = pd.DataFrame([[train_accuracy, train_f1, train_auc, train_precision, train_recall, train_specificity],[test_accuracy, test_f1, test_auc, test_precision, test_recall, test_specificity],[diff_accuracy, diff_f1, diff_auc, diff_precision, diff_recall, diff_specificity]],\n",
    "                              columns = ['Accuracy', 'F1', 'AUC', 'Precision', 'Recall', 'Specificity'],\n",
    "                              index = ['Train','Test', 'Diferencia'])\n",
    "\n",
    "    return metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_metrics(y_train, y_test, train_pred, test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización de la matriz de confusión\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(pd.crosstab(y_test, y_pred), annot=True, fmt='g', cmap='Blues')\n",
    "plt.xlabel('Predicciones')\n",
    "plt.ylabel('Valores verdaderos')\n",
    "plt.title('Matriz de confusión')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimización de hiperparámetros\n",
    "param_grid = {\n",
    "    'n_estimators': np.arange(100, 501, 50),\n",
    "    'learning_rate': np.linspace(0, 0.01, 6)\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Mejores hiperparámetros\n",
    "print(\"Mejores hiperparámetros:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicciones\n",
    "y_pred_opt = grid_search.predict(X_test)\n",
    "\n",
    "# Evaluación del modelo\n",
    "opt_accuracy = accuracy_score(y_test, y_pred_opt)\n",
    "opt_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar el segundo modelo optimizacion en entrenamiento y prueba\n",
    "test_pred_opt = grid_search.predict(X_test)\n",
    "train_pred_opt = grid_search.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_metrics(y_train, y_test, train_pred_opt, test_pred_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validación cruzada estratificada\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "\n",
    "# Definir el modelo con los hiperparámetros optimizados\n",
    "model = XGBClassifier(n_estimators=300, learning_rate=0.005, random_state=42)\n",
    "\n",
    "# Definir la estrategia de validación cruzada estratificada\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Calcular la precisión mediante validación cruzada\n",
    "scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Imprimir los puntajes de precisión para cada pliegue\n",
    "print(\"Precisión por pliegue:\", scores)\n",
    "\n",
    "# Calcular la precisión media y su desviación estándar\n",
    "print(\"Precisión media:\", scores.mean())\n",
    "print(\"Desviación estándar de la precisión:\", scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Los resultados de la validación cruzada estratificada muestran que el modelo tiene una precisión bastante consistente en cada pliegue,\n",
    "con puntajes que varían entre aproximadamente 0.743 y 0.786. La precisión media obtenida es de aproximadamente 0.768,\n",
    "lo que indica que el modelo tiene un buen rendimiento en general en la clasificación de los datos.\n",
    "\n",
    "La desviación estándar de la precisión es relativamente baja, alrededor de 0.014, lo que sugiere que los puntajes de precisión en los\n",
    "diferentes pliegues no difieren mucho entre sí, lo que indica una buena estabilidad en el rendimiento del modelo.\n",
    "En resumen, el modelo parece ser consistente y generaliza bien a través de los diferentes pliegues de la validación cruzada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización de las características más importantes\n",
    "feature_importance = pd.Series(grid_search.best_estimator_.feature_importances_, index=X.columns)\n",
    "feature_importance.nlargest(10).plot(kind='barh')\n",
    "plt.xlabel('Importancia')\n",
    "plt.ylabel('Características')\n",
    "plt.title('Características más importantes')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
